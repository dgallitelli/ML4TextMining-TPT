{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP4 - Non-negative Matrix Factorization\n",
    "The goal is to study the use of nonnegative matrix factorisation (NMF) for topic extraction from a dataset of text documents. The rationale is to interpret each extracted NMF component as being associated with a specific topic. \n",
    "\n",
    "Study and test the following script (introduced  on [scikit](http://scikit-learn.org/stable/auto_examples/applications/plot_topics_extraction_with_nmf_lda.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vectorizeFeatures(_vectorizer=None, _random_state=None):\n",
    "    # Set default params\n",
    "    if _vectorizer is None:\n",
    "        vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, max_features=1000, stop_words='english')\n",
    "    else:\n",
    "        vectorizer = _vectorizer\n",
    "    random_state = 1 if _random_state is None else _random_state\n",
    "    # Fetch data and vectorize\n",
    "    print(\"Loading dataset...\")\n",
    "    dataset = fetch_20newsgroups(shuffle=True, random_state=random_state,\n",
    "                                 remove=('headers', 'footers', 'quotes'))\n",
    "    data_samples = dataset.data[:2000]        \n",
    "    t0 = time()\n",
    "    features = vectorizer.fit_transform(data_samples)\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    print(\"done in %0.3fs.\" % (time() - t0))\n",
    "    return features, feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NMFModel(features, _vectorizerName=None, _random_state=None, \n",
    "             _beta_loss=None, _init=None, _W=None, _H=None, _K = None):\n",
    "    \n",
    "    n_samples = 2000\n",
    "    n_features = 1000\n",
    "    n_top_words = 20\n",
    "    n_components = 10 if _K is None else _K\n",
    "    vectorizerName = \"tf_idf\" if _vectorizerName is None else _vectorizerName\n",
    "    random_state = 1 if _random_state is None else _random_state\n",
    "    solver = 'cd' if _beta_loss is None else 'mu'\n",
    "    beta_loss = 'frobenius' if _beta_loss is None else _beta_loss\n",
    "    init = 'random' if _init is None else _init\n",
    "    \n",
    "    print(\"Fitting the NMF model (\"+beta_loss+\" norm) with \"+vectorizerName+\" features, \"\n",
    "          \"n_samples=%d and n_features=%d...\" % (n_samples, n_features))\n",
    "    \n",
    "    t0 = time()\n",
    "    if _init is None:\n",
    "        nmf = NMF(n_components=n_components, \n",
    "                  random_state=_random_state,\n",
    "                  solver = solver,\n",
    "                  beta_loss = beta_loss,\n",
    "                  init = 'random',\n",
    "                  alpha=.1, l1_ratio=.5).fit(features)\n",
    "    else:\n",
    "        nmf = NMF(n_components=n_components, \n",
    "                  random_state=_random_state,\n",
    "                  solver = solver,\n",
    "                  beta_loss = beta_loss,\n",
    "                  init = _init,\n",
    "                  alpha=.1, l1_ratio=.5)\n",
    "        nmf.fit_transform(features, W=_W, H=_H)\n",
    "    print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "    print(\"\\nTopics in NMF model (\"+beta_loss+\" norm):\")\n",
    "    return nmf, n_top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def runExample(_vectorizer=None, _vectorizerName=None, _random_state=None, _beta_loss=None, \n",
    "               _init=None, _W=None, _H=None, _K=None):\n",
    "    features, feature_names = vectorizeFeatures(_vectorizer, _random_state)\n",
    "    nmf, n_top_words = NMFModel(features, _vectorizerName, _random_state, _beta_loss, _init, _W, _H, _K)\n",
    "    print_top_words(nmf, feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1. Test and comment on the effect of varying the initialisation, especially using random nonnegative values as initial guesses (for W and H coefficients, using the notations introduced during the lecture).\n",
    "\n",
    "< to be rephrased >\n",
    "\n",
    "Three different initialisation configuration were tested, while mantaining the same cost function (frobenius norm) and using multiplicative update rules. The 'nndsvda' initializiation, as from the scikit documentation, performs a Nonnegative Double Singular Value Decomposition on the features and then fills zero values with the average value of the features matrix. The 'nndsvdar' initializiation performs the same operation, but fills zeros with very small random values. Finally, the random initialization creates two non-negative random matrices properly scaled.\n",
    "\n",
    "Of all the three approaches, the one that showed the best results was undoubtly the 'nndsvda', which was able to converge with only 30 iterations and provided the lowest error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 20news dataset. This may take a few minutes.\n",
      "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "done in 2.695s.\n",
      "Fitting the NMF model (frobenius norm) with tf_idf features, n_samples=2000 and n_features=1000...\n",
      "done in 0.549s.\n",
      "\n",
      "Topics in NMF model (frobenius norm):\n",
      "Topic #0: just people don like god good know time way say really make ve did ll want new right years year\n",
      "Topic #1: chip clipper going encryption yes clinton phone used enforcement serial encrypted knows block wonder email doesn market number follow standard\n",
      "Topic #2: law enforcement right gun crime weapons laws government away citizens people religious jesus block rights federal country fact police control\n",
      "Topic #3: think don win people extra early sold need toronto means sex actually just pretty happen david wasn mike use list\n",
      "Topic #4: windows file use dos drive using problem files program software pc window help card running os drivers disk version available\n",
      "Topic #5: 00 sale card condition 10 price offer 250 asking today new cd tape 50 15 equipment 20 contact 12 software\n",
      "Topic #6: thanks know does mail advance hi info interested email anybody looking like list send appreciated help information card need address\n",
      "Topic #7: key keys encryption government public use secure security communications nsa phone private legal court message allow encrypted enforcement house does\n",
      "Topic #8: car cars tires miles engine new insurance speed oil power price 000 good brake condition models used bought area year\n",
      "Topic #9: edu soon com send university internet mit ftp mail cc article information pub hope email home blood contact mac need\n",
      "\n"
     ]
    }
   ],
   "source": [
    "runExample(_init='random')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "done in 1.168s.\n",
      "Fitting the NMF model (frobenius norm) with tf_idf features, n_samples=2000 and n_features=1000...\n",
      "done in 0.702s.\n",
      "\n",
      "Topics in NMF model (frobenius norm):\n",
      "Topic #0: just people don think like know time good make way really say right ve want did ll new use years\n",
      "Topic #1: windows use dos using window program os drivers application help software pc running ms screen files version card code work\n",
      "Topic #2: god jesus bible faith christian christ christians does heaven sin believe lord life church mary atheism belief human love religion\n",
      "Topic #3: thanks know does mail advance hi info interested email anybody looking card help like appreciated information send list video need\n",
      "Topic #4: car cars 00 tires miles new engine insurance price condition oil power speed good 000 brake year used models bought\n",
      "Topic #5: edu soon com send university internet mit ftp mail cc pub article information hope program mac email home contact blood\n",
      "Topic #6: file problem files format win sound ftp pub read save site image help available create copy running memory self version\n",
      "Topic #7: game team games year win play season players nhl runs goal hockey toronto division flyers player defense leafs bad teams\n",
      "Topic #8: drive drives hard disk floppy software card mac computer power scsi controller apple mb 00 pc rom sale problem internal\n",
      "Topic #9: key chip clipper keys encryption government public use secure enforcement phone nsa communications law encrypted security clinton used legal standard\n",
      "\n"
     ]
    }
   ],
   "source": [
    "runExample(_init='nndsvda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "done in 1.434s.\n",
      "Fitting the NMF model (frobenius norm) with tf_idf features, n_samples=2000 and n_features=1000...\n",
      "done in 0.866s.\n",
      "\n",
      "Topics in NMF model (frobenius norm):\n",
      "Topic #0: just people don think like know time good make way really say right ve want did ll new use years\n",
      "Topic #1: windows use dos using window program os drivers application help software pc running ms screen files version card code work\n",
      "Topic #2: god jesus bible faith christian christ christians does heaven sin believe lord life church mary atheism belief human love religion\n",
      "Topic #3: thanks know does mail advance hi info interested email anybody looking card help like appreciated information send list video need\n",
      "Topic #4: car cars tires 00 miles new engine insurance price condition oil power speed good 000 brake year used models bought\n",
      "Topic #5: edu soon com send university internet mit ftp mail cc pub article information hope program mac email home contact blood\n",
      "Topic #6: file problem files format win sound ftp pub read save site image help available create copy running memory self version\n",
      "Topic #7: game team games year win play season players nhl runs goal hockey toronto division flyers player defense leafs bad teams\n",
      "Topic #8: drive drives hard disk floppy software card mac computer power scsi controller apple mb 00 pc rom sale problem internal\n",
      "Topic #9: key chip clipper keys encryption government public use secure enforcement phone nsa communications law encrypted security clinton used legal standard\n",
      "\n"
     ]
    }
   ],
   "source": [
    "runExample(_init='nndsvdar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2. Compare and comment on the difference between the results obtained with $l_2$ cost compared to the generalised Kullback-Liebler cost.\n",
    "\n",
    "< to be rephrased >\n",
    "\n",
    "The Kullback-Lieber cost seems to perform worse than the $ l_{2} $ cost: it needs 3.5 times the iterations to converge. Found topics seem to be similar (religion, business, games...), but also from this representation we can see that the l2 cost produces more accurate results: in topics obtained with l2 cost, all words within the same topic seem to be specific to it, whilst in the topics obtained with Kullback-Leibler cost there seems to be less precision. As an example, we can look at topic #2, where both algorithms find the same topic, but the most important words found by using the l2 cost are much more representative than the ones found using the Kullback-Leibler one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "done in 1.540s.\n",
      "Fitting the NMF model (kullback-leibler norm) with tf_idf features, n_samples=2000 and n_features=1000...\n",
      "done in 17.930s.\n",
      "\n",
      "Topics in NMF model (kullback-leibler norm):\n",
      "Topic #0: years case idea make guess say government did law think self talk way wrong talking control point deleted wasn short\n",
      "Topic #1: wrong sale year 20 offer world red sell 10 price 00 11 condition states 12 30 1993 st 13 50\n",
      "Topic #2: sure ve ll does mean year remember heard seen said bit doesn hear don true really say thing mentioned head\n",
      "Topic #3: work using help use works need write type good questions try read problem subject appreciated drive open return tried computer\n",
      "Topic #4: people god question far say time believe true making understand life actually yes today makes sense support religion university man\n",
      "Topic #5: good time maybe things thing kind usually bad times like didn day way make thinking look took use fact came\n",
      "Topic #6: windows looking thanks use mail file hi available program info version pc need used video mac running does card machine\n",
      "Topic #7: know think don want thanks people like let does need team win tell time way really play pretty second runs\n",
      "Topic #8: just new right old used doing like small use want car wondering little won state really working turn tell power\n",
      "Topic #9: edu interested thought set like send mail post soon public posting buy low weeks cost key wonder looks going chip\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dgallitelli\\AppData\\Local\\conda\\conda\\envs\\SD-TSIA214\\lib\\site-packages\\sklearn\\decomposition\\nmf.py:1035: ConvergenceWarning: Maximum number of iteration 200 reached. Increase it to improve convergence.\n",
      "  \" improve convergence.\" % max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "runExample(_beta_loss='kullback-leibler')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3. Test and comment on the results obtained using a simpler term-frequency representation as input (as opposed to the TF-IDF representation considered in the code above) when considering the Kullback-Liebler cost.\n",
    "\n",
    "< to be rephrased >\n",
    "\n",
    "Two simpler representations were tested, both the simple Term Frequency representation and the simple Count of tokens. However, neither of them seems to produce better results: if compared to the previous Tf-Idf representation, both of them need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "done in 1.234s.\n",
      "Fitting the NMF model (kullback-leibler norm) with CountVectorizer features, n_samples=2000 and n_features=1000...\n",
      "done in 11.983s.\n",
      "\n",
      "Topics in NMF model (kullback-leibler norm):\n",
      "Topic #0: new car people years 000 year hiv president health cars research number program time aids used insurance cost engine oil\n",
      "Topic #1: game year team play win points got season games flyers second years time players couple great mark good just point\n",
      "Topic #2: 10 11 55 15 20 12 25 18 00 17 16 13 21 19 24 14 22 23 93 40\n",
      "Topic #3: government law use state public encryption person section gun rights clipper used weapons crime people military control security enforcement self\n",
      "Topic #4: windows thanks using use file help time problem does software know window color new program need running hi work video\n",
      "Topic #5: god people does jesus did time say israel believe bible think just church world know fact says life point true\n",
      "Topic #6: edu com mail graphics send ftp pub available contact computer list data version ca machines faq message sun cs type\n",
      "Topic #7: drive hard disk drives chip card speed scsi controller 16 rom bios board power data floppy feature bit interface high\n",
      "Topic #8: people key didn space said went just know time like did came keys going don come years old took armenians\n",
      "Topic #9: don like just think know good want ve way really need make going doesn say thing sure ll work does\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=1000, stop_words='english')    \n",
    "runExample(_beta_loss='kullback-leibler', _vectorizer=_vectorizer, _vectorizerName=\"CountVectorizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____________________________________\n",
    "## Custom NFM Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###### CUSTOM NMF IMPLEMENTATION ######\n",
    "# Multiplicative Update Rules for NMF #\n",
    "# estimation with beta divergences    #\n",
    "import numpy\n",
    "\n",
    "# TODO: translate slides 59 [beta-divergence] & 47 [error and special cases]\n",
    "\n",
    "def custom_NMF(V, K, W=None, H=None, steps=50, beta=0, toll=0.1, show_div=False):\n",
    "    \n",
    "    F = len(V) #Number of V rows\n",
    "    N = len(V[0]) #Number of V columns\n",
    "\n",
    "    if W is None:\n",
    "        W = numpy.random.rand(F,K)\n",
    "        \n",
    "    if H is None:\n",
    "        H = numpy.random.rand(K,N)\n",
    "        \n",
    "    if N != len(H[0]):\n",
    "        raise ValueError(\"Size for H[0] is different - found \"+str(len(H[0]))+\" in place of \"+str(N))\n",
    "    if F != len(W):\n",
    "        raise ValueError(\"Size for F is different - found \"+str(len(F))+\" in place of \"+str(N))\n",
    "        \n",
    "    #Setup n_iter\n",
    "    n_iter = 1\n",
    "    \n",
    "    # Setup initial error\n",
    "    init_error = _beta_div(V,W,H,beta,F,N,K)\n",
    "    if show_div:\n",
    "        print(\"Initial error: \"+str(init_error))\n",
    "    error = init_error\n",
    "    \n",
    "    for step in range(steps):\n",
    "    \n",
    "#         Tests with whole matrix : multiply = O | dot = *\n",
    "        upd_UP = numpy.dot(W.T, numpy.multiply(pow(numpy.dot(W,H),beta-2), V))\n",
    "        upd_DOWN = numpy.dot(W.T, pow(numpy.dot(W,H),beta-1))\n",
    "        upd = upd_UP / upd_DOWN\n",
    "        H = numpy.multiply(H, upd)\n",
    "        \n",
    "        upd_UP = numpy.dot(numpy.multiply(pow(numpy.dot(W,H),beta-2), V),H.T)\n",
    "        upd_DOWN = numpy.dot(pow(numpy.dot(W,H),beta-1), H.T)\n",
    "        upd = upd_UP / upd_DOWN\n",
    "        W = numpy.multiply(W, upd)\n",
    "        \n",
    "        if toll > 0:\n",
    "            new_error = _beta_div(V,W,H,beta,F,N,K)\n",
    "            if show_div:\n",
    "                print(\"Error on iteration \"+str(n_iter)+\": \" +str(new_error))\n",
    "            # Check if approximation error relative decrease is below the desired threshold\n",
    "            if ((error - new_error) / init_error) < toll:\n",
    "                break\n",
    "            error = new_error\n",
    "            \n",
    "        n_iter += 1\n",
    "            \n",
    "    return W, H\n",
    "\n",
    "def _beta_div(V,W,H,beta,F,N,K):\n",
    "    div = 0\n",
    "    # Update beta_divergence\n",
    "    WH = numpy.dot(W, H)\n",
    "    for i in range(F):\n",
    "        for j in range(N):\n",
    "                x = V[i][j] if V[i][j] != 0 else numpy.finfo(numpy.double).tiny\n",
    "                y = WH[i][j]\n",
    "                if beta == 1: # generalized Kullback-Leibler divergence. x log(x/y) - x + y\n",
    "                    div += x*numpy.log(x/y) - x + y\n",
    "                elif beta == 0: # Itakura-Saito divergence. (x/y) - log(x/y) -1\n",
    "                    div += (x/y) * numpy.log(x/y) - 1\n",
    "                else: # Euclidean distance. (1/beta(beta-1))(x^beta + (beta-1)y^beta - beta*x*y^beta-1)\n",
    "                    div += 1/(beta*(beta-1))*(pow(x,beta) + (beta-1)*pow(y,beta) - beta*x*pow(y,beta-1))\n",
    "    return div\n",
    "\n",
    "#######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "done in 1.587s.\n",
      "Initial error: 2675399.9859820143\n",
      "Error on iteration 1: 198321.5349171468\n",
      "Error on iteration 2: 197666.45467101078\n"
     ]
    }
   ],
   "source": [
    "features, feature_names = vectorizeFeatures()\n",
    "\n",
    "V = numpy.random.rand(features.shape[0], features.shape[1])\n",
    "V = numpy.array(V) # Data matrix F x N \n",
    "K = 10\n",
    "\n",
    "W, H = custom_NMF(V, K, beta = 1, toll = 0.001, show_div = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "done in 1.380s.\n",
      "Fitting the NMF model (frobenius norm) with tf_idf features, n_samples=2000 and n_features=1000...\n",
      "done in 0.041s.\n",
      "\n",
      "Topics in NMF model (frobenius norm):\n",
      "Topic #0: young encrypted exactly evidence events especially error eric equipment entire engine enforcement energy end encryption email drive effort effective effect\n",
      "Topic #1: young encrypted exactly evidence events especially error eric equipment entire engine enforcement energy end encryption email drive effort effective effect\n",
      "Topic #2: young encrypted exactly evidence events especially error eric equipment entire engine enforcement energy end encryption email drive effort effective effect\n",
      "Topic #3: young encrypted exactly evidence events especially error eric equipment entire engine enforcement energy end encryption email drive effort effective effect\n",
      "Topic #4: young encrypted exactly evidence events especially error eric equipment entire engine enforcement energy end encryption email drive effort effective effect\n",
      "Topic #5: young encrypted exactly evidence events especially error eric equipment entire engine enforcement energy end encryption email drive effort effective effect\n",
      "Topic #6: young encrypted exactly evidence events especially error eric equipment entire engine enforcement energy end encryption email drive effort effective effect\n",
      "Topic #7: young encrypted exactly evidence events especially error eric equipment entire engine enforcement energy end encryption email drive effort effective effect\n",
      "Topic #8: young encrypted exactly evidence events especially error eric equipment entire engine enforcement energy end encryption email drive effort effective effect\n",
      "Topic #9: just like don know people think does good use time new god way ve make want thanks need really say\n",
      "\n"
     ]
    }
   ],
   "source": [
    "runExample(_init='custom', _W=W, _H=H, _K=K)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
