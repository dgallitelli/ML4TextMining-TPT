{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP4 - Non-negative Matrix Factorization\n",
    "The goal is to study the use of nonnegative matrix factorisation (NMF) for topic extraction from a dataset of text documents. The rationale is to interpret each extracted NMF component as being associated with a specific topic. \n",
    "\n",
    "Study and test the following script (introduced  on [scikit](http://scikit-learn.org/stable/auto_examples/applications/plot_topics_extraction_with_nmf_lda.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorizeFeatures(_vectorizer=None, _random_state=None):\n",
    "    # Set default params\n",
    "    if _vectorizer is None:\n",
    "        vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, max_features=1000, stop_words='english')\n",
    "    else:\n",
    "        vectorizer = _vectorizer\n",
    "    random_state = 1 if _random_state is None else _random_state\n",
    "    # Fetch data and vectorize\n",
    "    print(\"Loading dataset...\")\n",
    "    dataset = fetch_20newsgroups(shuffle=True, random_state=random_state,\n",
    "                                 remove=('headers', 'footers', 'quotes'))\n",
    "    data_samples = dataset.data[:2000]        \n",
    "    t0 = time()\n",
    "    features = vectorizer.fit_transform(data_samples)\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    print(\"done in %0.3fs.\" % (time() - t0))\n",
    "    return features, feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NMFModel(features, _vectorizerName=None, _random_state=None, \n",
    "             _beta_loss=None, _init=None, _W=None, _H=None, _K = None):\n",
    "    \n",
    "    n_samples = 2000\n",
    "    n_features = 1000\n",
    "    n_top_words = 20\n",
    "    n_components = 10 if _K is None else _K\n",
    "    vectorizerName = \"tf_idf\" if _vectorizerName is None else _vectorizerName\n",
    "    random_state = 1 if _random_state is None else _random_state\n",
    "    solver = 'cd' if _beta_loss is None else 'mu'\n",
    "    beta_loss = 'frobenius' if _beta_loss is None else _beta_loss\n",
    "    init = 'random' if _init is None else _init\n",
    "    \n",
    "    print(\"Fitting the NMF model (\"+beta_loss+\" norm) with \"+vectorizerName+\" features, \"\n",
    "          \"n_samples=%d and n_features=%d...\" % (n_samples, n_features))\n",
    "    \n",
    "    t0 = time()\n",
    "    if _init is None:\n",
    "        nmf = NMF(n_components=n_components, \n",
    "                  random_state=_random_state,\n",
    "                  solver = solver,\n",
    "                  beta_loss = beta_loss,\n",
    "                  alpha=.1, l1_ratio=.5).fit(features)\n",
    "    else:\n",
    "        nmf = NMF(n_components=n_components, \n",
    "                  random_state=_random_state,\n",
    "                  solver = solver,\n",
    "                  beta_loss = beta_loss,\n",
    "                  alpha=.1, l1_ratio=.5).fit_transform(features, W=_W, H=_H)\n",
    "    print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "    print(\"\\nTopics in NMF model (\"+beta_loss+\" norm):\")\n",
    "    return nmf, n_top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runExample(_vectorizer=None, _vectorizerName=None, _random_state=None, _beta_loss=None, \n",
    "               _init=None, _W=None, _H=None, _K=None):\n",
    "    features, feature_names = vectorizeFeatures()\n",
    "    nmf, n_top_words = NMFModel(features, _vectorizerName, _random_state, _beta_loss, _init, _W, _H, _K)\n",
    "    print_top_words(nmf, feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1. Test and comment on the effect of varying the initialisation, especially using random nonnegative values as initial guesses (for W and H coefficients, using the notations introduced during the lecture)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TfidfVectorizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-62a9c77afd05>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrunExample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-92faef5b7f3a>\u001b[0m in \u001b[0;36mrunExample\u001b[0;34m(_vectorizer, _vectorizerName, _random_state, _beta_loss, _init, _W, _H, _K)\u001b[0m\n\u001b[1;32m      1\u001b[0m def runExample(_vectorizer=None, _vectorizerName=None, _random_state=None, _beta_loss=None, \n\u001b[1;32m      2\u001b[0m                _init=None, _W=None, _H=None, _K=None):\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizeFeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mnmf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_top_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNMFModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_vectorizerName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_random_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_beta_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_init\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_W\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_H\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_K\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint_top_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnmf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_top_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-417364a9c21a>\u001b[0m in \u001b[0;36mvectorizeFeatures\u001b[0;34m(_vectorizer, _random_state)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# Set default params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_vectorizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mvectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.95\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mvectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_vectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TfidfVectorizer' is not defined"
     ]
    }
   ],
   "source": [
    "runExample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runExample(_random_state=29)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runExample(_random_state=69)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2. Compare and comment on the difference between the results obtained with 2 cost compared to the generalised Kullback-Liebler cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runExample(_beta_loss='kullback-leibler')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3. Test and comment on the results obtained using a simpler term-frequency representation as input (as opposed to the TF-IDF representation considered in the code above) when considering the Kullback-Liebler cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=1000, stop_words='english')    \n",
    "runExample(_beta_loss='kullback-leibler', _vectorizer=_vectorizer, _vectorizerName=\"CountVectorizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____________________________________\n",
    "## Custom NFM Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### CUSTOM NMF IMPLEMENTATION ######\n",
    "# Multiplicative Update Rules for NMF #\n",
    "# estimation with beta divergences    #\n",
    "import numpy\n",
    "\n",
    "# TODO: translate slides 59 [beta-divergence] & 47 [error and special cases]\n",
    "\n",
    "def custom_NMF(V, K, W=None, H=None, steps=50, beta=0, toll=0.1, show_div=False):\n",
    "    \n",
    "    F = len(V) #Number of V rows\n",
    "    N = len(V[0]) #Number of V columns\n",
    "\n",
    "    if W is None:\n",
    "        W = numpy.random.rand(F,K)\n",
    "        \n",
    "    if H is None:\n",
    "        H = numpy.random.rand(K,N)\n",
    "        \n",
    "    if N != len(H[0]):\n",
    "        raise ValueError(\"Size for H[0] is different - found \"+str(len(H[0]))+\" in place of \"+str(N))\n",
    "    if F != len(W):\n",
    "        raise ValueError(\"Size for F is different - found \"+str(len(F))+\" in place of \"+str(N))\n",
    "        \n",
    "    #Setup n_iter\n",
    "    n_iter = 1\n",
    "    \n",
    "    # Setup initial error\n",
    "    init_error = _beta_div(V,W,H,beta,F,N,K)\n",
    "    if show_div:\n",
    "        print(\"Initial error: \"+str(init_error))\n",
    "    error = init_error\n",
    "    \n",
    "    for step in range(steps):\n",
    "    \n",
    "#         Tests with whole matrix : multiply = O | dot = *\n",
    "        upd_UP = numpy.dot(W.T, numpy.multiply(pow(numpy.dot(W,H),beta-2), V))\n",
    "        upd_DOWN = numpy.dot(W.T, pow(numpy.dot(W,H),beta-1))\n",
    "        upd = upd_UP / upd_DOWN\n",
    "        H = numpy.multiply(H, upd)\n",
    "        \n",
    "        upd_UP = numpy.dot(numpy.multiply(pow(numpy.dot(W,H),beta-2), V),H.T)\n",
    "        upd_DOWN = numpy.dot(pow(numpy.dot(W,H),beta-1), H.T)\n",
    "        upd = upd_UP / upd_DOWN\n",
    "        W = numpy.multiply(W, upd)\n",
    "        \n",
    "        if toll > 0:\n",
    "            new_error = _beta_div(V,W,H,beta,F,N,K)\n",
    "            if show_div:\n",
    "                print(\"Error on iteration \"+str(n_iter)+\": \" +str(new_error))\n",
    "            # Check if approximation error relative decrease is below the desired threshold\n",
    "            if ((error - new_error) / init_error) < toll:\n",
    "                break\n",
    "            error = new_error\n",
    "            \n",
    "        n_iter += 1\n",
    "            \n",
    "    return W, H\n",
    "\n",
    "def _beta_div(V,W,H,beta,F,N,K):\n",
    "    div = 0\n",
    "    # Update beta_divergence\n",
    "    WH = numpy.dot(W, H)\n",
    "    for i in range(F):\n",
    "        for j in range(N):\n",
    "                x = V[i][j] if V[i][j] != 0 else numpy.finfo(numpy.double).tiny\n",
    "                y = WH[i][j]\n",
    "                if beta == 1: # generalized Kullback-Leibler divergence. x log(x/y) - x + y\n",
    "                    div += x*numpy.log(x/y) - x + y\n",
    "                elif beta == 0: # Itakura-Saito divergence. (x/y) - log(x/y) -1\n",
    "                    div += (x/y) * numpy.log(x/y) - 1\n",
    "                else: # Euclidean distance. (1/beta(beta-1))(x^beta + (beta-1)y^beta - beta*x*y^beta-1)\n",
    "                    div += 1/(beta*(beta-1))*(pow(x,beta) + (beta-1)*pow(y,beta) - beta*x*pow(y,beta-1))\n",
    "    return div\n",
    "\n",
    "#######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, feature_names = vectorizeFeatures()\n",
    "\n",
    "V = numpy.random.rand(features.shape[0], features.shape[1])\n",
    "V = numpy.array(V) # Data matrix F x N \n",
    "K = 2\n",
    "\n",
    "W, H = custom_NMF(V, K, beta = 1, toll = 0.001, show_div = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runExample(_init='custom', _W=W, _H=H)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
