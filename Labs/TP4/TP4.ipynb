{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP4 - Non-negative Matrix Factorization\n",
    "The goal is to study the use of nonnegative matrix factorisation (NMF) for topic extraction from a dataset of text documents. The rationale is to interpret each extracted NMF component as being associated with a specific topic. \n",
    "\n",
    "Study and test the following script (introduced  on [scikit](http://scikit-learn.org/stable/auto_examples/applications/plot_topics_extraction_with_nmf_lda.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vectorizeFeatures(_vectorizer=None, _random_state=None):\n",
    "    # Set default params\n",
    "    if _vectorizer is None:\n",
    "        vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, max_features=1000, stop_words='english')\n",
    "    else:\n",
    "        vectorizer = _vectorizer\n",
    "    random_state = 1 if _random_state is None else _random_state\n",
    "    # Fetch data and vectorize\n",
    "    print(\"Loading dataset...\")\n",
    "    dataset = fetch_20newsgroups(shuffle=True, random_state=random_state,\n",
    "                                 remove=('headers', 'footers', 'quotes'))\n",
    "    data_samples = dataset.data[:2000]        \n",
    "    t0 = time()\n",
    "    features = vectorizer.fit_transform(data_samples)\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    print(\"done in %0.3fs.\" % (time() - t0))\n",
    "    return features, feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NMFModel(features, _vectorizerName=None, _random_state=None, \n",
    "             _beta_loss=None, _init=None, _W=None, _H=None, _K = None):\n",
    "    \n",
    "    n_samples = 2000\n",
    "    n_features = 1000\n",
    "    n_top_words = 20\n",
    "    n_components = 10 if _K is None else _K\n",
    "    vectorizerName = \"tf_idf\" if _vectorizerName is None else _vectorizerName\n",
    "    random_state = 1 if _random_state is None else _random_state\n",
    "    solver = 'mu'\n",
    "    beta_loss = 'frobenius' if _beta_loss is None else _beta_loss\n",
    "    init = 'random' if _init is None else _init\n",
    "    \n",
    "    print(\"Fitting the NMF model (\"+beta_loss+\" norm) with \"+vectorizerName+\" features, \"\n",
    "          \"n_samples=%d and n_features=%d...\" % (n_samples, n_features))\n",
    "    \n",
    "    t0 = time()\n",
    "    if _init is None:\n",
    "        nmf = NMF(n_components=n_components, \n",
    "                  random_state=_random_state,\n",
    "                  solver = solver,\n",
    "                  beta_loss = beta_loss,\n",
    "                  init = 'random',\n",
    "                  alpha=.1, l1_ratio=.5).fit(features)\n",
    "    else:\n",
    "        nmf = NMF(n_components=n_components, \n",
    "                  random_state=_random_state,\n",
    "                  solver = solver,\n",
    "                  beta_loss = beta_loss,\n",
    "                  init = _init,\n",
    "                  alpha=.1, l1_ratio=.5)\n",
    "        nmf.fit_transform(features, W=_W, H=_H)\n",
    "    print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "    print(\"\\nTopics in NMF model (\"+beta_loss+\" norm):\")\n",
    "    return nmf, n_top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def runExample(_vectorizer=None, _vectorizerName=None, _random_state=None, _beta_loss=None, \n",
    "               _init=None, _W=None, _H=None, _K=None):\n",
    "    features, feature_names = vectorizeFeatures(_vectorizer, _random_state)\n",
    "    nmf, n_top_words = NMFModel(features, _vectorizerName, _random_state, _beta_loss, _init, _W, _H, _K)\n",
    "    print_top_words(nmf, feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1. Test and comment on the effect of varying the initialisation, especially using random nonnegative values as initial guesses (for W and H coefficients, using the notations introduced during the lecture).\n",
    "\n",
    "The NMF Model in Scikit-Learn allows for different possible initialisations, among which three have been tested, all of them having by default a **TF-IDF Vectorizer**, the **same cost function** (*frobenius* $l_2$ normalization), and all of them using **multiplicative update (MU) rules**:\n",
    "\n",
    "| Init | Convergence Time (seconds) |\n",
    "|------|------------------|\n",
    "|Random|0.119|\n",
    "|nndvsda|0.088|\n",
    "|nndvsdar|0.173|\n",
    "\n",
    "From the scikit documentation it is possible to understand that the *nndvsda* (Nonnegative Double Singular Value Decomposition) performs a decomposition on the features and then fills in zero values with the average value of the features matrix; the *nndsvdar* initializiation performs the same operation, but fills zeros with very small random values.\n",
    "\n",
    "The best model, in terms of convergence, was the *nndvsda* one, however the *random* one is very close."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "done in 0.429s.\n",
      "Fitting the NMF model (frobenius norm) with tf_idf features, n_samples=2000 and n_features=1000...\n",
      "done in 0.119s.\n",
      "\n",
      "Topics in NMF model (frobenius norm):\n",
      "Topic #0: car cars tires miles insurance engine oil speed 000 price new condition power brake good models buying area driver bought\n",
      "Topic #1: edu soon com send internet university mit mail cc article ftp hope information email blood pub home contact cs need\n",
      "Topic #2: just thought wondering sure listen does wrong mean bad argument heard oh book driving ll really like doesn want isn\n",
      "Topic #3: don know think like need look read mean pretty want really thinking does ve sure doesn probably bible newsgroup getting\n",
      "Topic #4: like time year good new ll game years way 10 got better team great didn space little really did make\n",
      "Topic #5: use window windows want using problem standard need hardware try good application work stuff power used doing available instead display\n",
      "Topic #6: people government law rights israel think say true evidence did make person right crime jews point mr state believe said\n",
      "Topic #7: thanks windows file does mail know card advance hi help dos info files looking program anybody ftp software using pc\n",
      "Topic #8: god jesus bible faith does christ christian christians heaven sin believe life lord church mary love human atheism religion belief\n",
      "Topic #9: key chip clipper keys encryption government public secure enforcement phone nsa communications encrypted security clinton law going message used legal\n",
      "\n"
     ]
    }
   ],
   "source": [
    "runExample(_init='random')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "done in 0.301s.\n",
      "Fitting the NMF model (frobenius norm) with tf_idf features, n_samples=2000 and n_features=1000...\n",
      "done in 0.088s.\n",
      "\n",
      "Topics in NMF model (frobenius norm):\n",
      "Topic #0: just people don think like know good time make way really say right ve want did ll new going years\n",
      "Topic #1: windows use dos using window program card help software pc drivers os application video running looking screen version graphics files\n",
      "Topic #2: god jesus bible faith christian christ does christians heaven sin believe lord life church mary atheism belief human love religion\n",
      "Topic #3: thanks know does advance mail info hi interested email anybody like list send information appreciated looking address post new reply\n",
      "Topic #4: 00 sale car 10 condition price card new offer 250 asking 15 12 20 50 today cd 30 contact sell\n",
      "Topic #5: edu soon com send university internet mit ftp mail cc article information pub hope mac home blood email contact need\n",
      "Topic #6: file files problem format win sound read pub ftp save create running site self copy image windows public available help\n",
      "Topic #7: game team games year win play season players nhl runs goal hockey toronto division flyers player defense leafs bad teams\n",
      "Topic #8: drive drives hard disk floppy mac software mb controller scsi computer rom apple power internal problems problem cable digital western\n",
      "Topic #9: key chip clipper keys encryption government public use secure enforcement phone nsa encrypted communications law security clinton used message standard\n",
      "\n"
     ]
    }
   ],
   "source": [
    "runExample(_init='nndsvda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "done in 0.396s.\n",
      "Fitting the NMF model (frobenius norm) with tf_idf features, n_samples=2000 and n_features=1000...\n",
      "done in 0.173s.\n",
      "\n",
      "Topics in NMF model (frobenius norm):\n",
      "Topic #0: just people don think like good know time make way really say right ve want did ll new going years\n",
      "Topic #1: windows use dos using window program card help software pc drivers os application video running screen version looking files graphics\n",
      "Topic #2: god jesus bible faith christian christ christians does heaven sin believe lord life church mary atheism belief human love religion\n",
      "Topic #3: thanks know does advance mail info hi interested email anybody like list send looking information appreciated address new post reply\n",
      "Topic #4: 00 sale car 10 condition price card new offer 250 asking 15 today 12 50 cd 20 interested 30 contact\n",
      "Topic #5: edu soon com send university internet mit ftp mail cc article information pub hope mac email home blood contact need\n",
      "Topic #6: file files problem format win sound read pub ftp save site create running self image copy public help available formats\n",
      "Topic #7: game team games year win play season players nhl runs goal hockey toronto division flyers player defense leafs bad teams\n",
      "Topic #8: drive drives hard disk floppy mac software mb controller scsi computer rom apple power internal problems problem cable digital access\n",
      "Topic #9: key chip clipper keys encryption government public use secure enforcement phone nsa encrypted communications law security clinton used message user\n",
      "\n"
     ]
    }
   ],
   "source": [
    "runExample(_init='nndsvdar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2. Compare and comment on the difference between the results obtained with $l_2$ cost compared to the generalised Kullback-Liebler cost.\n",
    "\n",
    "The generalised Kullback-Lieber cost seems to be outperformed by the $ l_{2} $ cost, since it takes way longer to reach convergence (from ~0.15s to > 3s), and the topics extracted with Kullback-Leibler cost seems to be less precise. For example in topics #4 and #8, the words extracted with KL cost do not give very much information regarding the topics - while in previous tests it was possible to infer more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "done in 0.317s.\n",
      "Fitting the NMF model (kullback-leibler norm) with tf_idf features, n_samples=2000 and n_features=1000...\n",
      "done in 3.332s.\n",
      "\n",
      "Topics in NMF model (kullback-leibler norm):\n",
      "Topic #0: thanks know need like want mail post send edu does list use time don information buy questions help reply let\n",
      "Topic #1: using drive new used use version sale need machine card work video pc memory data software problem monitor high computer\n",
      "Topic #2: think people wrong just god guess time believe person fact don really saying know agree understand important wouldn actually way\n",
      "Topic #3: com won 20 team number haven short 10 st second news 30 media free 1993 12 let president 14 toronto\n",
      "Topic #4: say does read like thought know probably interested want come wondering point don try just kind thinking subject article tell\n",
      "Topic #5: years got way old usually good edu just soon time ago couple maybe case ll low run talk doing like\n",
      "Topic #6: yes true mean things work stuff doesn don know different good heard matter mind want place make just unless course\n",
      "Topic #7: year sure look just trying good time better don using hear left car far said start use seen head new\n",
      "Topic #8: right thing world make people like government law question use number given possible way control oh numbers wonder tell write\n",
      "Topic #9: windows email file ve program looking hi remember mail help work window sun programs ftp edu internet space info science\n",
      "\n"
     ]
    }
   ],
   "source": [
    "runExample(_beta_loss='kullback-leibler')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3. Test and comment on the results obtained using a simpler term-frequency representation as input (as opposed to the TF-IDF representation considered in the code above) when considering the Kullback-Liebler cost.\n",
    "\n",
    "In the following test, the simpler CountVectorizer method was used for the term-frequency representation. Unlike TF-IDF (default in the above test), CountVectorizer reaches convergence in the default 200 max iterations, even though taking longer than previous tests with $l_2$ convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "done in 0.309s.\n",
      "Fitting the NMF model (kullback-leibler norm) with CountVectorizer features, n_samples=2000 and n_features=1000...\n",
      "done in 2.209s.\n",
      "\n",
      "Topics in NMF model (kullback-leibler norm):\n",
      "Topic #0: use windows thanks using does know problem help card need new file scsi window work time 00 color software like\n",
      "Topic #1: said people didn went children came hiv did told time took home started women new killed saw like happened building\n",
      "Topic #2: drive space disk hard drives israel controller rom earth bios data 16 floppy moon probe card lunar feature surface mission\n",
      "Topic #3: government key public use law state chip encryption clipper keys gun president used weapons security control crime states people enforcement\n",
      "Topic #4: edu com mail send list news server faq information message david xfree86 thanks post copy mit email questions posted article\n",
      "Topic #5: god does people jesus law believe bible church true person fact life point christian jews religion says read say section\n",
      "Topic #6: don like think just know people ve good want way really time say make ll going sure thing things let\n",
      "Topic #7: 10 55 11 game team play 12 15 20 18 period 25 17 13 19 14 22 year 24 23\n",
      "Topic #8: car year just good power right bike better cars use new used speed point oil light engine high got time\n",
      "Topic #9: graphics available edu ftp contact pub program version computer university mail file software research machines image source data science ray\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=1000, stop_words='english')    \n",
    "runExample(_beta_loss='kullback-leibler', _vectorizer=_vectorizer, _vectorizerName=\"CountVectorizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____________________________________\n",
    "## Custom NFM Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###### CUSTOM NMF IMPLEMENTATION ######\n",
    "# Multiplicative Update Rules for NMF #\n",
    "# estimation with beta divergences    #\n",
    "import numpy\n",
    "\n",
    "def custom_NMF(V, K, W=None, H=None, steps=50, beta=0, toll=0.1, show_div=False):\n",
    "    \n",
    "    F = len(V) #Number of V rows\n",
    "    N = len(V[0]) #Number of V columns\n",
    "\n",
    "    if W is None:\n",
    "        W = numpy.random.rand(F,K)\n",
    "        \n",
    "    if H is None:\n",
    "        H = numpy.random.rand(K,N)\n",
    "        \n",
    "    if N != len(H[0]):\n",
    "        raise ValueError(\"Size for H[0] is different - found \"+str(len(H[0]))+\" in place of \"+str(N))\n",
    "    if F != len(W):\n",
    "        raise ValueError(\"Size for F is different - found \"+str(len(F))+\" in place of \"+str(N))\n",
    "        \n",
    "    #Setup n_iter\n",
    "    n_iter = 1\n",
    "    \n",
    "    # Setup initial error\n",
    "    init_error = _beta_div(V,W,H,beta,F,N,K)\n",
    "    if show_div:\n",
    "        print(\"Initial error: \"+str(init_error))\n",
    "    error = init_error\n",
    "    \n",
    "    for step in range(steps):\n",
    "    \n",
    "#         Tests with whole matrix : multiply = O | dot = *\n",
    "        upd_UP = numpy.dot(W.T, numpy.multiply(pow(numpy.dot(W,H),beta-2), V))\n",
    "        upd_DOWN = numpy.dot(W.T, pow(numpy.dot(W,H),beta-1))\n",
    "        upd = upd_UP / upd_DOWN\n",
    "        H = numpy.multiply(H, upd)\n",
    "        \n",
    "        upd_UP = numpy.dot(numpy.multiply(pow(numpy.dot(W,H),beta-2), V),H.T)\n",
    "        upd_DOWN = numpy.dot(pow(numpy.dot(W,H),beta-1), H.T)\n",
    "        upd = upd_UP / upd_DOWN\n",
    "        W = numpy.multiply(W, upd)\n",
    "        \n",
    "        if toll > 0:\n",
    "            new_error = _beta_div(V,W,H,beta,F,N,K)\n",
    "            if show_div:\n",
    "                print(\"Error on iteration \"+str(n_iter)+\": \" +str(new_error))\n",
    "            # Check if approximation error relative decrease is below the desired threshold\n",
    "            if ((error - new_error) / init_error) < toll:\n",
    "                break\n",
    "            error = new_error\n",
    "            \n",
    "        n_iter += 1\n",
    "            \n",
    "    return W, H\n",
    "\n",
    "def _beta_div(V,W,H,beta,F,N,K):\n",
    "    div = 0\n",
    "    # Update beta_divergence\n",
    "    WH = numpy.dot(W, H)\n",
    "    for i in range(F):\n",
    "        for j in range(N):\n",
    "                x = V[i][j] if V[i][j] != 0 else numpy.finfo(numpy.double).tiny\n",
    "                y = WH[i][j]\n",
    "                if beta == 1: # generalized Kullback-Leibler divergence. x log(x/y) - x + y\n",
    "                    div += x*numpy.log(x/y) - x + y\n",
    "                elif beta == 0: # Itakura-Saito divergence. (x/y) - log(x/y) -1\n",
    "                    div += (x/y) * numpy.log(x/y) - 1\n",
    "                else: # Euclidean distance. (1/beta(beta-1))(x^beta + (beta-1)y^beta - beta*x*y^beta-1)\n",
    "                    div += 1/(beta*(beta-1))*(pow(x,beta) + (beta-1)*pow(y,beta) - beta*x*pow(y,beta-1))\n",
    "    return div\n",
    "\n",
    "#######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "done in 0.323s.\n",
      "Initial error: 2598546.0529280994\n",
      "Error on iteration 1: 198049.876475166\n",
      "Error on iteration 2: 197385.47350395098\n",
      "Error on iteration 3: 196824.9661585167\n",
      "Error on iteration 4: 196345.73798660949\n",
      "Error on iteration 5: 195931.30470955608\n",
      "Error on iteration 6: 195569.33383094586\n",
      "Error on iteration 7: 195250.4176342084\n",
      "Error on iteration 8: 194967.25614096617\n",
      "Error on iteration 9: 194714.09903612413\n"
     ]
    }
   ],
   "source": [
    "features, feature_names = vectorizeFeatures()\n",
    "\n",
    "V = numpy.random.rand(features.shape[0], features.shape[1])\n",
    "V = numpy.array(V) # Data matrix F x N \n",
    "K = 10\n",
    "\n",
    "W, H = custom_NMF(V, K, beta = 1, toll = 0.0001, show_div = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the NMF model (frobenius norm) with tf_idf features, n_samples=2000 and n_features=1000...\n",
      "done in 0.065s.\n",
      "\n",
      "Topics in NMF model (frobenius norm):\n",
      "Topic #0: god jesus bible faith does christian christ christians heaven sin believe lord life mary love church atheism human belief religion\n",
      "Topic #1: windows file dos files program problem using os help drivers running ftp ms version application window screen pc programs mode\n",
      "Topic #2: use drive new key car good software power chip used computer using need speed want 00 high disk drives card\n",
      "Topic #3: think don good need win extra book does did make case pretty course yes try means actually early guess sold\n",
      "Topic #4: game team games year win play season players nhl runs goal hockey division toronto player flyers defense bad time won\n",
      "Topic #5: thanks know does advance mail hi info interested anybody email looking help card appreciated information list send need video address\n",
      "Topic #6: edu soon com send university internet mit mail ftp cc article pub hope information home contact blood program email cs\n",
      "Topic #7: just thought bike don sure wondering listen new bad ll really heard car driving wrong oh mean got argument right\n",
      "Topic #8: like don sounds know look looks thing make way newsgroup got ve doing mean great say good post isn parts\n",
      "Topic #9: people know don government say time right law really did let said going make way ve point didn ll believe\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nmf, n_top_words = NMFModel(features, _init='custom', _W=W, _H=H, _K=K)\n",
    "print_top_words(nmf, feature_names, n_top_words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
